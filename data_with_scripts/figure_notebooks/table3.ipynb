{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,roc_auc_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all=list(range(129))\n",
    "cdr1 = list(range(25, 40 + 1))\n",
    "cdr2=list(range(54, 67 + 1))\n",
    "cdr3=list(range(103, 119 + 1))\n",
    "cdrs = cdr1 + cdr2 + cdr3\n",
    "\n",
    "def get_f1_scores(predictions, column=\"prediction\", threshold1=0.5, threshold2=0.5):\n",
    "    # Filter predictions where IMGT_bis is in cdrs\n",
    "    cdr_predictions = predictions.query(\"IMGT_bis in @cdrs\")\n",
    "    non_cdr_predictions = predictions.query(\"IMGT_bis not in @cdrs\")\n",
    "\n",
    "    # Apply threshold1 to cdr_predictions\n",
    "    cdr_preds = (cdr_predictions[column] >= threshold1).astype(int).tolist()\n",
    "    cdr_labs = cdr_predictions[\"labels\"].tolist()\n",
    "\n",
    "    # Apply threshold2 to non_cdr_predictions\n",
    "    non_cdr_preds = (non_cdr_predictions[column] >= threshold2).astype(int).tolist()\n",
    "    non_cdr_labs = non_cdr_predictions[\"labels\"].tolist()\n",
    "\n",
    "    # Combine predictions and labels\n",
    "    all_preds = cdr_preds + non_cdr_preds\n",
    "    all_labs = cdr_labs + non_cdr_labs\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(all_labs, all_preds)\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def get_mcc_scores(predictions, column=\"prediction\", threshold1=0.5,threshold2=0.5):\n",
    "    # Filter predictions where IMGT_bis is in cdrs\n",
    "    cdr_predictions = predictions.query(\"IMGT_bis in @cdrs\")\n",
    "    non_cdr_predictions = predictions.query(\"IMGT_bis not in @cdrs\")\n",
    "\n",
    "    # Apply threshold1 to cdr_predictions\n",
    "    cdr_preds = (cdr_predictions[column] >= threshold1).astype(int).tolist()\n",
    "    cdr_labs = cdr_predictions[\"labels\"].tolist()\n",
    "\n",
    "    # Apply threshold2 to non_cdr_predictions\n",
    "    non_cdr_preds = (non_cdr_predictions[column] >= threshold2).astype(int).tolist()\n",
    "    non_cdr_labs = non_cdr_predictions[\"labels\"].tolist()\n",
    "\n",
    "    # Combine predictions and labels\n",
    "    all_preds = cdr_preds + non_cdr_preds\n",
    "    all_labs = cdr_labs + non_cdr_labs\n",
    "    mcc = matthews_corrcoef(all_labs, all_preds)\n",
    "\n",
    "    return mcc\n",
    "def get_ap_scores(predictions, column=\"prediction\"):\n",
    "\n",
    "    preds = predictions[column].tolist()\n",
    "    labs = predictions[\"labels\"].tolist()\n",
    "    ap = average_precision_score(labs, preds)\n",
    "\n",
    "    return ap\n",
    "def get_roc_scores(predictions, column=\"prediction\"):\n",
    "    preds = predictions[column].tolist()\n",
    "    labs = predictions[\"labels\"].tolist()\n",
    "    roc = roc_auc_score(labs, preds)\n",
    "\n",
    "    return roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ap_roc_f1_mcc_df(llm_path1,llm_path2, threshold=0.5, column=\"labels_llm2\"):\n",
    "    predictions_llm1 = pd.read_csv(llm_path1)\n",
    "    predictions_llm2 = pd.read_csv(llm_path2)\n",
    "    predictions_llm = pd.merge(predictions_llm1, predictions_llm2[['pdb', 'IMGT', 'chain_type', 'prediction', 'labels']],\n",
    "                        on=[\"pdb\", \"IMGT\", \"chain_type\"],\n",
    "                        how=\"left\",\n",
    "                        suffixes=(\"_llm1\", \"_llm2\"))\n",
    "    predictions_llm=predictions_llm.dropna()\n",
    "    predictions_llm[\"labels\"]=predictions_llm[\"labels_llm1\"]\n",
    "    predictions_llm[\"prediction\"]=predictions_llm[column]\n",
    "    predictions_llm[\"IMGT_bis\"] = predictions_llm[\"IMGT\"].str.replace(r'[a-zA-Z]$', '', regex=True).astype(int)\n",
    "\n",
    "    # Compute all metrics\n",
    "\n",
    "    f1_dict = {\n",
    "    }\n",
    "\n",
    "    mcc_dict = {\n",
    "    }\n",
    "    f1_llm_list = []\n",
    "    mcc_llm_list = []\n",
    "\n",
    "    for _, df_pdb in predictions_llm.groupby(\"pdb\"):\n",
    "\n",
    "        f1_llm_list.append(get_f1_scores(df_pdb, column=\"prediction\", threshold1=threshold, threshold2=threshold))\n",
    "        mcc_llm_list.append(get_mcc_scores(df_pdb, column=\"prediction\", threshold1=threshold, threshold2=threshold))\n",
    "\n",
    "    # Average the results across pdb groups\n",
    "\n",
    "\n",
    "    f1_dict[\"llm\"] = np.mean(f1_llm_list)\n",
    "    mcc_dict[\"llm\"] = np.mean(mcc_llm_list)\n",
    "    mcc_dict[\"metric\"]=\"mcc\"\n",
    "    f1_dict[\"metric\"]=\"f1\"\n",
    "\n",
    "\n",
    "    return f1_dict, mcc_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             llm  seed\n",
      "metric                \n",
      "f1      0.953282   1.0\n",
      "mcc     0.949408   1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "records=[]\n",
    "\n",
    "for seed in tqdm(range(1, 2)):\n",
    "    llm_path1 = f\"/home/athenes/Paraplume/benchmark/paragraph/250526/lr-0.00005_dr-0.4,0.4,0.4_mk-0.4_bs-16_dim1-2000,1000,500_alphas-4,5,6_pen-0.00001_weight_1_emb_all_seed_{seed}/prediction_paragraph_test_limits1.csv\"\n",
    "    llm_path2 = f\"/home/athenes/Paraplume/benchmark/paragraph/250526/lr-0.00005_dr-0.4,0.4,0.4_mk-0.4_bs-16_dim1-2000,1000,500_alphas-4,5,6_pen-0.00001_weight_1_emb_all_seed_{seed}/prediction_paragraph_test_limits2.csv\"\n",
    "\n",
    "    f1_dict, mcc_dict = get_ap_roc_f1_mcc_df(llm_path1,llm_path2, threshold=0.5)\n",
    "    for each in [f1_dict, mcc_dict]:\n",
    "        each[\"seed\"]=seed\n",
    "        records.append(each)\n",
    "final_paragraph_limit = pd.DataFrame.from_records(records).groupby(\"metric\").mean()\n",
    "print(final_paragraph_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paraplume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:11<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             llm  seed\n",
      "metric                \n",
      "f1      0.710647   8.5\n",
      "mcc     0.686340   8.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "records=[]\n",
    "\n",
    "for seed in tqdm(range(1, 17)):\n",
    "    llm_path1 = f\"/home/athenes/Paraplume/benchmark/paragraph/250526/lr-0.00005_dr-0.4,0.4,0.4_mk-0.4_bs-16_dim1-2000,1000,500_alphas-4,5,6_pen-0.00001_weight_1_emb_all_seed_{seed}/prediction_paragraph_test_limits1.csv\"\n",
    "    llm_path2 = f\"/home/athenes/Paraplume/benchmark/paragraph/250526/lr-0.00005_dr-0.4,0.4,0.4_mk-0.4_bs-16_dim1-2000,1000,500_alphas-4,5,6_pen-0.00001_weight_1_emb_all_seed_{seed}/prediction_paragraph_test_limits2.csv\"\n",
    "\n",
    "    f1_dict, mcc_dict = get_ap_roc_f1_mcc_df(llm_path1,llm_path2, threshold=0.5, column=\"prediction_llm1\")\n",
    "    for each in [f1_dict, mcc_dict]:\n",
    "        each[\"seed\"]=seed\n",
    "        records.append(each)\n",
    "final_paragraph_model = pd.DataFrame.from_records(records).groupby(\"metric\").mean()\n",
    "print(final_paragraph_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIPE dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### uppder bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             llm  seed\n",
      "metric                \n",
      "f1      0.961295   1.0\n",
      "mcc     0.958439   1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "records=[]\n",
    "\n",
    "for seed in tqdm(range(1,2 )):\n",
    "    for cv in range(1):\n",
    "        llm_path1 = f\"/home/athenes/Paraplume/benchmark/mipe/250526/lr-0.00005_dr-0.4,0.4,0.4_mk-0.4_bs-16_dim1-2000,1000,500_alphas-4,5,6_pen-0.00001_weight_1_emb_all_seed_{seed}/{cv}/prediction_mipe_test_limits1.csv\"\n",
    "        llm_path2 = f\"/home/athenes/Paraplume/benchmark/mipe/250526/lr-0.00005_dr-0.4,0.4,0.4_mk-0.4_bs-16_dim1-2000,1000,500_alphas-4,5,6_pen-0.00001_weight_1_emb_all_seed_{seed}/{cv}/prediction_mipe_test_limits2.csv\"\n",
    "\n",
    "        f1_dict, mcc_dict = get_ap_roc_f1_mcc_df(llm_path1,llm_path2, threshold=0.5)\n",
    "        for each in [f1_dict, mcc_dict]:\n",
    "            each[\"seed\"]=seed\n",
    "            records.append(each)\n",
    "final_mipe_limit = pd.DataFrame.from_records(records).groupby(\"metric\").mean()\n",
    "print(final_mipe_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paraplume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:04<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             llm  seed\n",
      "metric                \n",
      "f1      0.711872   3.0\n",
      "mcc     0.689328   3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "records=[]\n",
    "\n",
    "for seed in tqdm(range(1,6)):\n",
    "    for cv in range(5):\n",
    "        llm_path1 = f\"/home/athenes/Paraplume/benchmark/mipe/250310_final/lr-0.00005_dr-0.4,0.4,0.4_mk-0.4_bs-16_dim1-2000,1000,500_alphas-4,5,6_pen-0.00001_weight_1_emb_all_seed_{seed}/{cv}/prediction_mipe_test_limits1.csv\"\n",
    "        llm_path2 = f\"/home/athenes/Paraplume/benchmark/mipe/250310_final/lr-0.00005_dr-0.4,0.4,0.4_mk-0.4_bs-16_dim1-2000,1000,500_alphas-4,5,6_pen-0.00001_weight_1_emb_all_seed_{seed}/{cv}/prediction_mipe_test_limits2.csv\"\n",
    "\n",
    "        f1_dict, mcc_dict = get_ap_roc_f1_mcc_df(llm_path1,llm_path2, threshold=0.5, column=\"prediction_llm1\")\n",
    "        for each in [f1_dict, mcc_dict]:\n",
    "            each[\"seed\"]=seed\n",
    "            records.append(each)\n",
    "final_mipe_model = pd.DataFrame.from_records(records).groupby(\"metric\").mean()\n",
    "print(final_mipe_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pecan dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             llm  seed\n",
      "metric                \n",
      "f1      0.947263   1.0\n",
      "mcc     0.943651   1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "records=[]\n",
    "\n",
    "for seed in tqdm(range(1, 2)):\n",
    "    llm_path1 = f\"/home/athenes/Paraplume/benchmark/pecan/250526/lr-0.00005_dr-0.4,0.4,0.4_mk-0.4_bs-16_dim1-2000,1000,500_alphas-4,5,6_pen-0.00001_weight_1_emb_all_seed_{seed}/prediction_pecan_test_limits1.csv\"\n",
    "    llm_path2 = f\"/home/athenes/Paraplume/benchmark/pecan/250526/lr-0.00005_dr-0.4,0.4,0.4_mk-0.4_bs-16_dim1-2000,1000,500_alphas-4,5,6_pen-0.00001_weight_1_emb_all_seed_{seed}/prediction_pecan_test_limits2.csv\"\n",
    "\n",
    "    f1_dict, mcc_dict = get_ap_roc_f1_mcc_df(llm_path1,llm_path2, threshold=0.5)\n",
    "    for each in [f1_dict, mcc_dict]:\n",
    "        each[\"seed\"]=seed\n",
    "        records.append(each)\n",
    "final_pecan_limit = pd.DataFrame.from_records(records).groupby(\"metric\").mean()\n",
    "print(final_pecan_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paraplume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             llm  seed\n",
      "metric                \n",
      "f1      0.662878   2.0\n",
      "mcc     0.637215   2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "records=[]\n",
    "\n",
    "for seed in tqdm(range(1, 4)):\n",
    "    llm_path1 = f\"/home/athenes/Paraplume/benchmark/pecan/250310_final/lr-0.00005_dr-0.4,0.4,0.4_mk-0.4_bs-16_dim1-2000,1000,500_alphas-4,5,6_pen-0.00001_weight_1_emb_all_seed_{seed}/prediction_pecan_test_limits1.csv\"\n",
    "    llm_path2 = f\"/home/athenes/Paraplume/benchmark/pecan/250310_final/lr-0.00005_dr-0.4,0.4,0.4_mk-0.4_bs-16_dim1-2000,1000,500_alphas-4,5,6_pen-0.00001_weight_1_emb_all_seed_{seed}/prediction_pecan_test_limits2.csv\"\n",
    "\n",
    "    f1_dict, mcc_dict = get_ap_roc_f1_mcc_df(llm_path1,llm_path2, threshold=0.5, column=\"prediction_llm1\")\n",
    "    for each in [f1_dict, mcc_dict]:\n",
    "        each[\"seed\"]=seed\n",
    "        records.append(each)\n",
    "final_pecan_model = pd.DataFrame.from_records(records).groupby(\"metric\").mean()\n",
    "print(final_pecan_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
