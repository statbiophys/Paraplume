{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,roc_auc_score\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdr1 = list(range(27, 38 + 1))\n",
    "cdr2=list(range(56, 65 + 1))\n",
    "cdr3=list(range(105, 117 + 1))\n",
    "cdrs = cdr1 + cdr2 + cdr3\n",
    "cdr_ranges = {\n",
    "    \"CDR1\": cdr1,\n",
    "    \"CDR2\": cdr2,\n",
    "    \"CDR3\": cdr3,\n",
    "}\n",
    "all=list(range(129))\n",
    "framework = [each for each in all if each not in cdrs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM / Paragraph / combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "def get_ap_roc_f1_mcc_df(llm_path):\n",
    "    llm = pd.read_csv(llm_path)\n",
    "    llm[\"IMGT_bis\"] = llm[\"IMGT\"].str.replace(r'[a-zA-Z]$', '', regex=True).astype(int)\n",
    "    ap_scores = {}\n",
    "    roc_scores = {}\n",
    "\n",
    "    for _, df_pdb in llm.groupby(\"pdb\"):\n",
    "        for cdr_name, cdr_range in cdr_ranges.items():\n",
    "            for chain in [\"light\", \"heavy\"]:\n",
    "                preds_cdr = df_pdb.query(\"IMGT_bis in @cdr_range and chain_type==@chain\")[\"prediction\"].tolist()\n",
    "                labs_cdr = df_pdb.query(\"IMGT_bis in @cdr_range and chain_type==@chain\")[\"labels\"].tolist()\n",
    "\n",
    "                if f\"{cdr_name} {chain}\" not in ap_scores:\n",
    "                    ap_scores[f\"{cdr_name} {chain}\"] = []\n",
    "                    roc_scores[f\"{cdr_name} {chain}\"] = []\n",
    "\n",
    "                if len(set(labs_cdr)) == 1:  # Check if all labels are the same\n",
    "                    roc_auc = 1.0\n",
    "                else:\n",
    "                    roc_auc = roc_auc_score(labs_cdr, preds_cdr)\n",
    "\n",
    "                ap_scores[f\"{cdr_name} {chain}\"].append(average_precision_score(labs_cdr, preds_cdr))\n",
    "                roc_scores[f\"{cdr_name} {chain}\"].append(roc_auc)\n",
    "\n",
    "        for name, range in zip([\"CDRs\", \"Framework\", \"Whole sequence\"], [cdrs, framework, all]):\n",
    "            preds_cdr = df_pdb.query(\"IMGT_bis in @range\")[\"prediction\"].tolist()\n",
    "            labs_cdr = df_pdb.query(\"IMGT_bis in @range\")[\"labels\"].tolist()\n",
    "\n",
    "            if name not in ap_scores:\n",
    "                ap_scores[name] = []\n",
    "            if name not in roc_scores:\n",
    "                roc_scores[name] = []\n",
    "\n",
    "            if len(set(labs_cdr)) == 1:  # Check if only one class exists\n",
    "                roc_auc = 1.0\n",
    "            else:\n",
    "                roc_auc = roc_auc_score(labs_cdr, preds_cdr)\n",
    "\n",
    "            ap_scores[name].append(average_precision_score(labs_cdr, preds_cdr))\n",
    "            roc_scores[name].append(roc_auc)\n",
    "\n",
    "    # Compute mean scores\n",
    "    for key in ap_scores:\n",
    "        ap_scores[key] = np.mean(ap_scores[key])\n",
    "    for key in roc_scores:\n",
    "        roc_scores[key] = np.mean(roc_scores[key])\n",
    "\n",
    "    return ap_scores, roc_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ap_roc_f1_mcc_df_one_hot(paragraph_path, llm_path=\"/home/athenes/Paraplume/benchmark/paragraph/250526/lr-0.00005_dr-0.4,0.4,0.4_mk-0.4_bs-16_dim1-2000,1000,500_alphas-4,5,6_pen-0.00001_weight_1_emb_all_seed_1/prediction_test_set.csv\"):\n",
    "    predictions_llm = pd.read_csv(llm_path)\n",
    "    predictions_llm[\"IMGT_bis\"] = predictions_llm[\"IMGT\"].str.replace(r'[a-zA-Z]$', '', regex=True).astype(int)\n",
    "    predictions_paragraph = pd.read_csv(paragraph_path)\n",
    "    predictions_paragraph[\"IMGT_bis\"] = predictions_paragraph[\"IMGT\"].str.replace(r'[a-zA-Z]$', '', regex=True).astype(int)\n",
    "\n",
    "    combined = pd.merge(predictions_llm, predictions_paragraph[['pdb', 'IMGT', 'chain_type', 'prediction']],\n",
    "                        on=[\"pdb\", \"IMGT\", \"chain_type\"],\n",
    "                        how=\"left\",\n",
    "                        suffixes=(\"_llm\", \"_paragraph\"))\n",
    "\n",
    "\n",
    "    combined[\"prediction\"] = np.where(\n",
    "        combined[\"prediction_paragraph\"].notna(),\n",
    "        combined[\"prediction_paragraph\"],\n",
    "        0\n",
    "    )\n",
    "\n",
    "    ap_scores = {}\n",
    "    roc_scores = {}\n",
    "\n",
    "    for _, df_pdb in combined.groupby(\"pdb\"):\n",
    "        for cdr_name, cdr_range in cdr_ranges.items():\n",
    "            for chain in [\"light\", \"heavy\"]:\n",
    "                preds_cdr = df_pdb.query(\"IMGT_bis in @cdr_range and chain_type==@chain\")[\"prediction\"].tolist()\n",
    "                labs_cdr = df_pdb.query(\"IMGT_bis in @cdr_range and chain_type==@chain\")[\"labels\"].tolist()\n",
    "\n",
    "                if f\"{cdr_name} {chain}\" not in ap_scores:\n",
    "                    ap_scores[f\"{cdr_name} {chain}\"] = []\n",
    "                    roc_scores[f\"{cdr_name} {chain}\"] = []\n",
    "\n",
    "                if len(set(labs_cdr)) == 1:  # Check if all labels are the same\n",
    "                    roc_auc = 1.0\n",
    "                else:\n",
    "                    roc_auc = roc_auc_score(labs_cdr, preds_cdr)\n",
    "\n",
    "                ap_scores[f\"{cdr_name} {chain}\"].append(average_precision_score(labs_cdr, preds_cdr))\n",
    "                roc_scores[f\"{cdr_name} {chain}\"].append(roc_auc)\n",
    "\n",
    "        for name, range in zip([\"CDRs\", \"Framework\", \"Whole sequence\"], [cdrs, framework, all]):\n",
    "            preds_cdr = df_pdb.query(\"IMGT_bis in @range\")[\"prediction\"].tolist()\n",
    "            labs_cdr = df_pdb.query(\"IMGT_bis in @range\")[\"labels\"].tolist()\n",
    "\n",
    "            if name not in ap_scores:\n",
    "                ap_scores[name] = []\n",
    "            if name not in roc_scores:\n",
    "                roc_scores[name] = []\n",
    "\n",
    "            if len(set(labs_cdr)) == 1:  # Check if only one class exists\n",
    "                roc_auc = 1.0\n",
    "            else:\n",
    "                roc_auc = roc_auc_score(labs_cdr, preds_cdr)\n",
    "\n",
    "            ap_scores[name].append(average_precision_score(labs_cdr, preds_cdr))\n",
    "            roc_scores[name].append(roc_auc)\n",
    "\n",
    "    # Compute mean scores\n",
    "    for key in ap_scores:\n",
    "        ap_scores[key] = np.mean(ap_scores[key])\n",
    "    for key in roc_scores:\n",
    "        roc_scores[key] = np.mean(roc_scores[key])\n",
    "\n",
    "    return ap_scores, roc_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# table s3, Paraplume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "records=[]\n",
    "for seed in range(1, 17):\n",
    "    llm_path = f\"/home/athenes/Paraplume/benchmark/paragraph/250526/lr-0.00005_dr-0.4,0.4,0.4_mk-0.4_bs-16_dim1-2000,1000,500_alphas-4,5,6_pen-0.00001_weight_1_emb_all_seed_{seed}/prediction_test_set.csv\"\n",
    "    ap_scores, roc_scores = get_ap_roc_f1_mcc_df(llm_path)\n",
    "    ap_scores[\"metric\"]=\"ap\"\n",
    "    roc_scores[\"metric\"]=\"roc\"\n",
    "    ap_scores[\"seed\"]=seed\n",
    "    roc_scores[\"seed\"]=seed\n",
    "    records.append(ap_scores)\n",
    "    records.append(roc_scores)\n",
    "df = pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        CDR1 light  CDR1 heavy  CDR2 light  CDR2 heavy  CDR3 light  \\\n",
      "metric                                                               \n",
      "ap        0.786264    0.789808    0.450736    0.804707    0.789705   \n",
      "roc       0.910678    0.922713    0.989536    0.931404    0.940954   \n",
      "\n",
      "        CDR3 heavy      CDRs  Framework  Whole sequence  seed  \n",
      "metric                                                         \n",
      "ap        0.838380  0.785496   0.667942        0.757530   8.5  \n",
      "roc       0.892584  0.872421   0.977329        0.966174   8.5  \n"
     ]
    }
   ],
   "source": [
    "print(df.groupby(\"metric\").mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# table s3, paragraph crystal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [02:02<00:00,  7.64s/it]\n"
     ]
    }
   ],
   "source": [
    "records=[]\n",
    "for seed in tqdm(range(1, 17)):\n",
    "    one_hot_path = f\"/home/athenes/Paraplume/benchmark/paragraph/3D/{seed}/prediction_.csv\"\n",
    "    ap_scores, roc_scores = get_ap_roc_f1_mcc_df_one_hot(one_hot_path)\n",
    "    ap_scores[\"metric\"]=\"ap\"\n",
    "    roc_scores[\"metric\"]=\"roc\"\n",
    "    ap_scores[\"seed\"]=seed\n",
    "    roc_scores[\"seed\"]=seed\n",
    "    records.append(ap_scores)\n",
    "    records.append(roc_scores)\n",
    "df = pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        CDR1 light  CDR1 heavy  CDR2 light  CDR2 heavy  CDR3 light  \\\n",
      "metric                                                               \n",
      "ap        0.799641    0.802783    0.451739    0.804122    0.808919   \n",
      "roc       0.928089    0.934435    0.990539    0.929676    0.953174   \n",
      "\n",
      "        CDR3 heavy      CDRs  Framework  Whole sequence  seed  \n",
      "metric                                                         \n",
      "ap        0.892786  0.821705   0.565694        0.769627   8.5  \n",
      "roc       0.921862  0.888321   0.831195        0.939005   8.5  \n"
     ]
    }
   ],
   "source": [
    "print(df.groupby(\"metric\").mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
