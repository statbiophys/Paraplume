{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,roc_auc_score\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all=list(range(129))\n",
    "cdr1 = list(range(25, 40 + 1))\n",
    "cdr2=list(range(54, 67 + 1))\n",
    "cdr3=list(range(103, 119 + 1))\n",
    "cdrs = cdr1 + cdr2 + cdr3\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def get_f1_scores(predictions, column=\"prediction\", threshold1=0.5, threshold2=0.5):\n",
    "    # Filter predictions where IMGT_bis is in cdrs\n",
    "    cdr_predictions = predictions.query(\"IMGT_bis in @cdrs\")\n",
    "    non_cdr_predictions = predictions.query(\"IMGT_bis not in @cdrs\")\n",
    "\n",
    "    # Apply threshold1 to cdr_predictions\n",
    "    cdr_preds = (cdr_predictions[column] >= threshold1).astype(int).tolist()\n",
    "    cdr_labs = cdr_predictions[\"labels\"].tolist()\n",
    "\n",
    "    # Apply threshold2 to non_cdr_predictions\n",
    "    non_cdr_preds = (non_cdr_predictions[column] >= threshold2).astype(int).tolist()\n",
    "    non_cdr_labs = non_cdr_predictions[\"labels\"].tolist()\n",
    "\n",
    "    # Combine predictions and labels\n",
    "    all_preds = cdr_preds + non_cdr_preds\n",
    "    all_labs = cdr_labs + non_cdr_labs\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(all_labs, all_preds)\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def get_mcc_scores(predictions, column=\"prediction\", threshold1=0.5,threshold2=0.5):\n",
    "    # Filter predictions where IMGT_bis is in cdrs\n",
    "    cdr_predictions = predictions.query(\"IMGT_bis in @cdrs\")\n",
    "    non_cdr_predictions = predictions.query(\"IMGT_bis not in @cdrs\")\n",
    "\n",
    "    # Apply threshold1 to cdr_predictions\n",
    "    cdr_preds = (cdr_predictions[column] >= threshold1).astype(int).tolist()\n",
    "    cdr_labs = cdr_predictions[\"labels\"].tolist()\n",
    "\n",
    "    # Apply threshold2 to non_cdr_predictions\n",
    "    non_cdr_preds = (non_cdr_predictions[column] >= threshold2).astype(int).tolist()\n",
    "    non_cdr_labs = non_cdr_predictions[\"labels\"].tolist()\n",
    "\n",
    "    # Combine predictions and labels\n",
    "    all_preds = cdr_preds + non_cdr_preds\n",
    "    all_labs = cdr_labs + non_cdr_labs\n",
    "    mcc = matthews_corrcoef(all_labs, all_preds)\n",
    "\n",
    "    return mcc\n",
    "\n",
    "\n",
    "def get_ap_scores(predictions, column=\"prediction\"):\n",
    "\n",
    "    preds = predictions[column].tolist()\n",
    "    labs = predictions[\"labels\"].tolist()\n",
    "    ap = average_precision_score(labs, preds)\n",
    "\n",
    "    return ap\n",
    "def get_roc_scores(predictions, column=\"prediction\"):\n",
    "    preds = predictions[column].tolist()\n",
    "    labs = predictions[\"labels\"].tolist()\n",
    "    roc = roc_auc_score(labs, preds)\n",
    "\n",
    "    return roc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM / Paragraph / combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ap_roc_f1_mcc_df(llm_path, paragraph_path, threshold1=0.5,threshold2=0.5):\n",
    "\n",
    "    predictions_llm = pd.read_csv(llm_path)\n",
    "    predictions_llm[\"IMGT_bis\"] = predictions_llm[\"IMGT\"].str.replace(r'[a-zA-Z]$', '', regex=True).astype(int)\n",
    "    predictions_paragraph = pd.read_csv(paragraph_path)\n",
    "    predictions_paragraph[\"IMGT_bis\"] = predictions_paragraph[\"IMGT\"].str.replace(r'[a-zA-Z]$', '', regex=True).astype(int)\n",
    "\n",
    "    combined = pd.merge(predictions_llm, predictions_paragraph[['pdb', 'IMGT', 'chain_type', 'prediction']],\n",
    "                        on=[\"pdb\", \"IMGT\", \"chain_type\"],\n",
    "                        how=\"left\",\n",
    "                        suffixes=(\"_llm\", \"_paragraph\"))\n",
    "\n",
    "    combined[\"prediction\"] = np.where(\n",
    "        combined[\"prediction_paragraph\"].notna(),\n",
    "        combined[\"prediction_paragraph\"],\n",
    "        combined[\"prediction_llm\"]\n",
    "    )\n",
    "    combined[\"prediction_paragraph\"] = np.where(\n",
    "        combined[\"prediction_paragraph\"].notna(),\n",
    "        combined[\"prediction_paragraph\"],\n",
    "        0\n",
    "    )\n",
    "\n",
    "    # Compute all metrics\n",
    "    ap_dict = {\n",
    "    }\n",
    "\n",
    "    roc_dict = {\n",
    "    }\n",
    "\n",
    "    f1_dict = {\n",
    "    }\n",
    "\n",
    "    mcc_dict = {\n",
    "    }\n",
    "\n",
    "    ap_paragraph_list = []\n",
    "    roc_paragraph_list = []\n",
    "    f1_paragraph_list = []\n",
    "    mcc_paragraph_list = []\n",
    "    ap_llm_list = []\n",
    "    roc_llm_list = []\n",
    "    f1_llm_list = []\n",
    "    mcc_llm_list = []\n",
    "    ap_combined_list = []\n",
    "    roc_combined_list = []\n",
    "    f1_combined_list = []\n",
    "    mcc_combined_list = []\n",
    "\n",
    "    for _, df_pdb in combined.groupby(\"pdb\"):\n",
    "        ap_llm_list.append(get_ap_scores(df_pdb, column=\"prediction_llm\"))\n",
    "        roc_llm_list.append(get_roc_scores(df_pdb, column=\"prediction_llm\"))\n",
    "        f1_llm_list.append(get_f1_scores(df_pdb, column=\"prediction_llm\", threshold1=threshold2, threshold2=threshold2))\n",
    "        mcc_llm_list.append(get_mcc_scores(df_pdb, column=\"prediction_llm\", threshold1=threshold2, threshold2=threshold2))\n",
    "        ap_paragraph_list.append(get_ap_scores(df_pdb, column=\"prediction_paragraph\"))\n",
    "        roc_paragraph_list.append(get_roc_scores(df_pdb, column=\"prediction_paragraph\"))\n",
    "        f1_paragraph_list.append(get_f1_scores(df_pdb, column=\"prediction_paragraph\", threshold1=threshold1, threshold2=threshold1))\n",
    "        mcc_paragraph_list.append(get_mcc_scores(df_pdb, column=\"prediction_paragraph\", threshold1=threshold1, threshold2=threshold1))\n",
    "        ap_combined_list.append(get_ap_scores(df_pdb, column=\"prediction\"))\n",
    "        roc_combined_list.append(get_roc_scores(df_pdb, column=\"prediction\"))\n",
    "        f1_combined_list.append(get_f1_scores(df_pdb, column=\"prediction\", threshold1=threshold1, threshold2=threshold2))\n",
    "        mcc_combined_list.append(get_mcc_scores(df_pdb, column=\"prediction\", threshold1=threshold1, threshold2=threshold2))\n",
    "\n",
    "    # Average the results across pdb groups\n",
    "    ap_dict[\"Combined\"] = np.mean(ap_combined_list)\n",
    "    roc_dict[\"Combined\"] = np.mean(roc_combined_list)\n",
    "    f1_dict[\"Combined\"] = np.mean(f1_combined_list)\n",
    "    mcc_dict[\"Combined\"] = np.mean(mcc_combined_list)\n",
    "    ap_dict[\"paragraph\"] = np.mean(ap_paragraph_list)\n",
    "    roc_dict[\"paragraph\"] = np.mean(roc_paragraph_list)\n",
    "    f1_dict[\"paragraph\"] = np.mean(f1_paragraph_list)\n",
    "    mcc_dict[\"paragraph\"] = np.mean(mcc_paragraph_list)\n",
    "    ap_dict[\"llm\"] = np.mean(ap_llm_list)\n",
    "    roc_dict[\"llm\"] = np.mean(roc_llm_list)\n",
    "    f1_dict[\"llm\"] = np.mean(f1_llm_list)\n",
    "    mcc_dict[\"llm\"] = np.mean(mcc_llm_list)\n",
    "    mcc_dict[\"metric\"]=\"mcc\"\n",
    "    f1_dict[\"metric\"]=\"f1\"\n",
    "    ap_dict[\"metric\"]=\"ap\"\n",
    "    roc_dict[\"metric\"]=\"roc\"\n",
    "\n",
    "\n",
    "    return ap_dict, roc_dict, f1_dict, mcc_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PECAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:03<00:00,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Combined  paragraph       llm  seed\n",
      "metric                                     \n",
      "ap      0.771552   0.754739  0.730465   8.5\n",
      "f1      0.696988   0.694492  0.681540   8.5\n",
      "mcc     0.674910   0.671838  0.656573   8.5\n",
      "roc     0.964925   0.940270  0.963271   8.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "records=[]\n",
    "\n",
    "for seed in tqdm(range(1, 17)):\n",
    "    llm_path = f\"/home/athenes/Paraplume/benchmark/pecan/250526/lr-0.00005_dr-0.4,0.4,0.4_mk-0.4_bs-16_dim1-2000,1000,500_alphas-4,5,6_pen-0.00001_weight_1_emb_all_seed_{seed}/prediction_test_set.csv\"\n",
    "    paragraph_path = f\"/home/athenes/Paraplume/benchmark/pecan/3D/{seed}/prediction_.csv\"\n",
    "\n",
    "    ap_dict, roc_dict, f1_dict, mcc_dict = get_ap_roc_f1_mcc_df(llm_path, paragraph_path, threshold1=0.5, threshold2=0.5)\n",
    "    for each in [ap_dict, roc_dict, f1_dict, mcc_dict]:\n",
    "        each[\"seed\"]=seed\n",
    "        records.append(each)\n",
    "\n",
    "final_pecan = pd.DataFrame.from_records(records).groupby(\"metric\").mean()\n",
    "print(final_pecan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:30<00:00,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Combined  paragraph       llm  seed\n",
      "metric                                     \n",
      "ap      0.790771   0.769627  0.757530   8.5\n",
      "f1      0.704156   0.699992  0.700768   8.5\n",
      "mcc     0.683468   0.678120  0.676167   8.5\n",
      "roc     0.968110   0.939005  0.966174   8.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "records=[]\n",
    "\n",
    "for seed in tqdm(range(1, 17)):\n",
    "    llm_path = f\"/home/athenes/Paraplume/benchmark/paragraph/250526/lr-0.00005_dr-0.4,0.4,0.4_mk-0.4_bs-16_dim1-2000,1000,500_alphas-4,5,6_pen-0.00001_weight_1_emb_all_seed_{seed}/prediction_test_set.csv\"\n",
    "    paragraph_path = f\"/home/athenes/Paraplume/benchmark/paragraph/3D/{seed}/prediction_.csv\"\n",
    "\n",
    "    ap_dict, roc_dict, f1_dict, mcc_dict = get_ap_roc_f1_mcc_df(llm_path, paragraph_path, threshold1=0.5, threshold2=0.5)\n",
    "    for each in [ap_dict, roc_dict, f1_dict, mcc_dict]:\n",
    "        each[\"seed\"]=seed\n",
    "        records.append(each)\n",
    "final_paragraph = pd.DataFrame.from_records(records).groupby(\"metric\").mean()\n",
    "print(final_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:40<00:00,  8.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Combined  paragraph       llm  seed   cv\n",
      "metric                                          \n",
      "ap      0.753108   0.742379  0.715676   3.0  2.0\n",
      "f1      0.662914   0.663246  0.651207   3.0  2.0\n",
      "mcc     0.648357   0.648532  0.632226   3.0  2.0\n",
      "roc     0.963797   0.943297  0.962228   3.0  2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "records=[]\n",
    "\n",
    "for seed in tqdm(range(1, 6)):\n",
    "    for cv in range(5):\n",
    "        llm_path = f\"/home/athenes/Paraplume/benchmark/mipe/250526/lr-0.00005_dr-0.4,0.4,0.4_mk-0.4_bs-16_dim1-2000,1000,500_alphas-4,5,6_pen-0.00001_weight_1_emb_all_seed_{seed}/{cv}/prediction_test_set.csv\"\n",
    "        paragraph_path = f\"/home/athenes/Paraplume/benchmark/mipe/3D/seed{seed}/{cv}/prediction_.csv\"\n",
    "\n",
    "        ap_dict, roc_dict, f1_dict, mcc_dict = get_ap_roc_f1_mcc_df(llm_path, paragraph_path, threshold1=0.5, threshold2=0.5)\n",
    "        for each in [ap_dict, roc_dict, f1_dict, mcc_dict]:\n",
    "            each[\"seed\"]=seed\n",
    "            each[\"cv\"]=cv\n",
    "            records.append(each)\n",
    "\n",
    "final_mipe = pd.DataFrame.from_records(records).groupby(\"metric\").mean()\n",
    "print(final_mipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIPE, paragraph method only with abb3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:40<00:00,  8.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric\n",
      "ap     0.689409\n",
      "f1     0.617336\n",
      "mcc    0.596099\n",
      "roc    0.936910\n",
      "Name: paragraph, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "records=[]\n",
    "\n",
    "for seed in tqdm(range(1, 6)):\n",
    "    for cv in range(5):\n",
    "        llm_path = f\"/home/athenes/Paraplume/benchmark/mipe/250526/lr-0.00005_dr-0.4,0.4,0.4_mk-0.4_bs-16_dim1-2000,1000,500_alphas-4,5,6_pen-0.00001_weight_1_emb_all_seed_{seed}/{cv}/prediction_test_set.csv\"\n",
    "        paragraph_path = f\"/home/athenes/Paraplume/benchmark/mipe/3D/seed{seed}/{cv}/prediction_abb3.csv\"\n",
    "\n",
    "        ap_dict, roc_dict, f1_dict, mcc_dict = get_ap_roc_f1_mcc_df(llm_path, paragraph_path, threshold1=0.734, threshold2=0.5)\n",
    "        for each in [ap_dict, roc_dict, f1_dict, mcc_dict]:\n",
    "            each[\"seed\"]=seed\n",
    "            each[\"cv\"]=cv\n",
    "            records.append(each)\n",
    "\n",
    "final_mipe = pd.DataFrame.from_records(records).groupby(\"metric\").mean()\n",
    "print(final_mipe[\"paragraph\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIPE, paragraph method with crystal structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:40<00:00,  8.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric\n",
      "ap     0.742379\n",
      "f1     0.651322\n",
      "mcc    0.634459\n",
      "roc    0.943297\n",
      "Name: paragraph, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "records=[]\n",
    "\n",
    "for seed in tqdm(range(1, 6)):\n",
    "    for cv in range(5):\n",
    "        llm_path = f\"/home/athenes/Paraplume/benchmark/mipe/250526/lr-0.00005_dr-0.4,0.4,0.4_mk-0.4_bs-16_dim1-2000,1000,500_alphas-4,5,6_pen-0.00001_weight_1_emb_all_seed_{seed}/{cv}/prediction_test_set.csv\"\n",
    "        paragraph_path = f\"/home/athenes/Paraplume/benchmark/mipe/3D/seed{seed}/{cv}/prediction_.csv\"\n",
    "\n",
    "        ap_dict, roc_dict, f1_dict, mcc_dict = get_ap_roc_f1_mcc_df(llm_path, paragraph_path, threshold1=0.734, threshold2=0.5)\n",
    "        for each in [ap_dict, roc_dict, f1_dict, mcc_dict]:\n",
    "            each[\"seed\"]=seed\n",
    "            each[\"cv\"]=cv\n",
    "            records.append(each)\n",
    "\n",
    "final_mipe = pd.DataFrame.from_records(records).groupby(\"metric\").mean()\n",
    "print(final_mipe[\"paragraph\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
