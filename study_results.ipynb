{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdr1 = list(range(27, 38 + 1))\n",
    "cdr2=list(range(56, 65 + 1))\n",
    "cdr3=list(range(105, 117 + 1))\n",
    "cdrs = cdr1 + cdr2 + cdr3\n",
    "def get_ap_scores(predictions):\n",
    "    cdr_ranges = {\n",
    "    \"cdr1\": cdr1,\n",
    "    \"cdr2\": cdr2,\n",
    "    \"cdr3\": cdr3\n",
    "    }\n",
    "    ap_scores = {}\n",
    "\n",
    "    for cdr_name, cdr_range in cdr_ranges.items():\n",
    "        preds_cdr = predictions.query(\"IMGT_bis in @cdr_range and chain_type=='light'\")[\"prediction\"].tolist()\n",
    "        labs_cdr = predictions.query(\"IMGT_bis in @cdr_range and chain_type=='light'\")[\"labels\"].tolist()\n",
    "        ap_scores[cdr_name] = average_precision_score(labs_cdr, preds_cdr)\n",
    "\n",
    "    # Display the AP scores for each CDR\n",
    "    print(\"Average Precision Scores by CDR:\")\n",
    "    for cdr, score in ap_scores.items():\n",
    "        print(f\"{cdr} light: {score}\")\n",
    "\n",
    "    # Step 2: Calculate AP score for each CDR\n",
    "    for cdr_name, cdr_range in cdr_ranges.items():\n",
    "        preds_cdr = predictions.query(\"IMGT_bis in @cdr_range and chain_type=='heavy'\")[\"prediction\"].tolist()\n",
    "        labs_cdr = predictions.query(\"IMGT_bis in @cdr_range and chain_type=='heavy'\")[\"labels\"].tolist()\n",
    "        ap_scores[cdr_name] = average_precision_score(labs_cdr, preds_cdr)\n",
    "    return ap_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_llm = pd.read_csv(\"/home/gathenes/paragraph_benchmark/convex_hull/z_lr-0.00001_dr-0.4,0.3,0.2_mk-0.4_bs-15_dim1-4000,2000,500_alphas-5_pen-0_weight_1_--bigembedding_multi__/prediction.csv\")\n",
    "predictions_llm[\"IMGT_bis\"] = predictions_llm[\"IMGT\"].str.replace(r'[a-zA-Z]$', '', regex=True).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cdrs=predictions_llm.query(\"IMGT_bis in @cdrs\")[\"prediction\"].tolist()\n",
    "labs_cdrs=predictions_llm.query(\"IMGT_bis in @cdrs\")[\"labels\"].tolist()\n",
    "preds_framework=predictions_llm.query(\"IMGT_bis not in @cdrs\")[\"prediction\"].tolist()\n",
    "labs_framework=(predictions_llm.query(\"IMGT_bis not in @cdrs\")[\"labels\"]).tolist()\n",
    "total_preds = preds_cdrs+preds_framework\n",
    "total_labs=labs_cdrs+labs_framework\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7270427331778189 0.7589673838367047 0.5186822220129452\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ap = average_precision_score(total_labs, total_preds)\n",
    "ap_cdrs = average_precision_score(labs_cdrs, preds_cdrs)\n",
    "ap_framework = average_precision_score(np.array(labs_framework), np.array(preds_framework))\n",
    "print(ap, ap_cdrs, ap_framework)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Scores by CDR:\n",
      "cdr1 light: 0.7586930793695318\n",
      "cdr2 light: 0.5805366580313819\n",
      "cdr3 light: 0.7587022461526179\n",
      "Average Precision Scores by CDR:\n",
      "cdr1 heavy: 0.7270926177009996\n",
      "cdr2 heavy: 0.7990506958411646\n",
      "cdr3 heavy: 0.7542836285197275\n"
     ]
    }
   ],
   "source": [
    "# Display the AP scores for each CDR\n",
    "ap_scores = get_ap_scores(predictions_llm)\n",
    "print(\"Average Precision Scores by CDR:\")\n",
    "for cdr, score in ap_scores.items():\n",
    "    print(f\"{cdr} heavy: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_paragraph = pd.read_csv(\"/home/gathenes/paragraph_benchmark/paragraph_weights/prediction.csv\")\n",
    "predictions_paragraph[\"IMGT_bis\"] = predictions_paragraph[\"IMGT\"].str.replace(r'[a-zA-Z]$', '', regex=True).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Scores by CDR:\n",
      "cdr1 light: 0.7604520415881645\n",
      "cdr2 light: 0.7133327813052373\n",
      "cdr3 light: 0.7887800763706876\n",
      "Average Precision Scores by CDR:\n",
      "cdr1 heavy: 0.7529291932397588\n",
      "cdr2 heavy: 0.8019699167499572\n",
      "cdr3 heavy: 0.8173583479659525\n"
     ]
    }
   ],
   "source": [
    "# Display the AP scores for each CDR\n",
    "ap_scores = get_ap_scores(predictions_paragraph)\n",
    "print(\"Average Precision Scores by CDR:\")\n",
    "for cdr, score in ap_scores.items():\n",
    "    print(f\"{cdr} heavy: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_llm_paragraph = predictions_llm\n",
    "df_merged = pd.merge(predictions_llm_paragraph, predictions_paragraph[['pdb', 'IMGT', 'chain_type', 'prediction']],\n",
    "                    on=[\"pdb\", \"IMGT\", \"chain_type\"],\n",
    "                    how=\"left\",\n",
    "                    suffixes=(\"_llm\", \"_paragraph\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming df_merged is the DataFrame you have after merging\n",
    "df_merged[\"prediction\"] = np.where(\n",
    "    df_merged[\"prediction_paragraph\"].notna(),\n",
    "    df_merged[\"prediction_paragraph\"],\n",
    "    df_merged[\"prediction_llm\"]\n",
    ")\n",
    "df_merged[\"prediction_paragraph_extended\"] = np.where(\n",
    "    df_merged[\"prediction_paragraph\"].notna(),\n",
    "    df_merged[\"prediction_paragraph\"],\n",
    "    0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_combined = average_precision_score(df_merged[\"labels\"].to_list(), df_merged[\"prediction\"].to_list())\n",
    "ap_combined_cdrs = average_precision_score(df_merged.query(\"IMGT_bis in @cdrs\")[\"labels\"].to_list(), df_merged.query(\"IMGT_bis in @cdrs\")[\"prediction\"].to_list())\n",
    "ap_combined_framework = average_precision_score(df_merged.query(\"IMGT_bis not in @cdrs\")[\"labels\"].to_list(), df_merged.query(\"IMGT_bis not in @cdrs\")[\"prediction\"].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7544987340781626 0.7895647302238212 0.5396840569636212\n"
     ]
    }
   ],
   "source": [
    "print(ap_combined, ap_combined_cdrs, ap_combined_framework)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_paragraph = average_precision_score(df_merged[\"labels\"].to_list(), df_merged[\"prediction_paragraph_extended\"].to_list())\n",
    "ap_paragraph_cdrs = average_precision_score(df_merged.query(\"IMGT_bis in @cdrs\")[\"labels\"].to_list(), df_merged.query(\"IMGT_bis in @cdrs\")[\"prediction_paragraph_extended\"].to_list())\n",
    "ap_paragraph_framework = average_precision_score(df_merged.query(\"IMGT_bis not in @cdrs\")[\"labels\"].to_list(), df_merged.query(\"IMGT_bis not in @cdrs\")[\"prediction_paragraph_extended\"].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7328245237447634 0.789636884600756 0.4225746917471489\n"
     ]
    }
   ],
   "source": [
    "print(ap_paragraph, ap_paragraph_cdrs, ap_paragraph_framework)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Scores by CDR:\n",
      "cdr1 light: 0.7603917973614088\n",
      "cdr2 light: 0.7133327813052373\n",
      "cdr3 light: 0.7887800763706876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Scores by CDR:\n",
      "cdr1 heavy: 0.7523288755290967\n",
      "cdr2 heavy: 0.8019699167499572\n",
      "cdr3 heavy: 0.8173583479659525\n"
     ]
    }
   ],
   "source": [
    "# Display the AP scores for each CDR\n",
    "ap_scores = get_ap_scores(df_merged)\n",
    "print(\"Average Precision Scores by CDR:\")\n",
    "for cdr, score in ap_scores.items():\n",
    "    print(f\"{cdr} heavy: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph with ground truth crystals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8072495191873574\n"
     ]
    }
   ],
   "source": [
    "predictions = pd.read_csv(\"/home/gathenes/paragraph_benchmark/paragraph_weights_2/prediction.csv\")\n",
    "predictions[\"IMGT_bis\"] = predictions[\"IMGT\"].str.replace(r'[a-zA-Z]$', '', regex=True).astype(int)\n",
    "preds_cdrs=predictions.query(\"IMGT_bis in @cdrs\")[\"prediction\"].tolist()\n",
    "labs_cdrs=predictions.query(\"IMGT_bis in @cdrs\")[\"labels\"].tolist()\n",
    "ap_cdrs = average_precision_score(labs_cdrs, preds_cdrs)\n",
    "print(ap_cdrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Scores by CDR:\n",
      "cdr1 light: 0.7804499187675555\n",
      "cdr2 light: 0.7216139782939159\n",
      "cdr3 light: 0.7995423740975024\n",
      "Average Precision Scores by CDR:\n",
      "cdr1 heavy: 0.7841651978049836\n",
      "cdr2 heavy: 0.8072048851438248\n",
      "cdr3 heavy: 0.8423764009933188\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# Assuming `predictions` DataFrame is already defined\n",
    "\n",
    "# Step 1: Convert \"IMGT\" column to numeric after removing letters\n",
    "predictions[\"IMGT_bis\"] = predictions[\"IMGT\"].str.replace(r'[a-zA-Z]$', '', regex=True).astype(int)\n",
    "\n",
    "# Define CDR ranges\n",
    "cdr_ranges = {\n",
    "    \"cdr1\": cdr1,\n",
    "    \"cdr2\": cdr2,\n",
    "    \"cdr3\": cdr3\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store AP scores for each CDR\n",
    "ap_scores = {}\n",
    "\n",
    "for cdr_name, cdr_range in cdr_ranges.items():\n",
    "    preds_cdr = predictions.query(\"IMGT_bis in @cdr_range and chain_type=='light'\")[\"prediction\"].tolist()\n",
    "    labs_cdr = predictions.query(\"IMGT_bis in @cdr_range and chain_type=='light'\")[\"labels\"].tolist()\n",
    "    ap_scores[cdr_name] = average_precision_score(labs_cdr, preds_cdr)\n",
    "\n",
    "# Display the AP scores for each CDR\n",
    "print(\"Average Precision Scores by CDR:\")\n",
    "for cdr, score in ap_scores.items():\n",
    "    print(f\"{cdr} light: {score}\")\n",
    "\n",
    "# Step 2: Calculate AP score for each CDR\n",
    "for cdr_name, cdr_range in cdr_ranges.items():\n",
    "    preds_cdr = predictions.query(\"IMGT_bis in @cdr_range and chain_type=='heavy'\")[\"prediction\"].tolist()\n",
    "    labs_cdr = predictions.query(\"IMGT_bis in @cdr_range and chain_type=='heavy'\")[\"labels\"].tolist()\n",
    "    ap_scores[cdr_name] = average_precision_score(labs_cdr, preds_cdr)\n",
    "\n",
    "# Display the AP scores for each CDR\n",
    "print(\"Average Precision Scores by CDR:\")\n",
    "for cdr, score in ap_scores.items():\n",
    "    print(f\"{cdr} heavy: {score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
