{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "\n",
    "# Load ESM-2 model\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "# Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)\n",
    "data = [\n",
    "    (\"protein1\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"),\n",
    "    (\"protein2\", \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "    (\"protein3\",  \"K A <mask> I S Q\"),\n",
    "]\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "# Extract per-residue representations (on CPU)\n",
    "with torch.no_grad():\n",
    "    results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "token_representations = results[\"representations\"][33]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 73, 1280])\n"
     ]
    }
   ],
   "source": [
    "print(token_representations.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antiberty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gathenes/.pyenv/versions/3.10.13/envs/embeddings_310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from antiberty import AntiBERTyRunner\n",
    "\n",
    "antiberty = AntiBERTyRunner()\n",
    "\n",
    "sequences = [\n",
    "    \"EVQLVQSGPEVKKPGTSVKVSCKASGFTFMSSAVQWVRQARGQRLEWIGWIVIGSGNTNYAQKFQERVTITRDMSTSTAYMELSSLRSEDTAVYYCAAPYCSSISCNDGFDIWGQGTMVTVS\",\n",
    "    \"DVVMTQTPFSLPVSLGDQASISCRSSQSLVHSNGNTYLHWYLQKPGQSPKLLIYKVSNRFSGVPDRFSGSGSGTDFTLKISRVEAEDLGVYFCSQSTHVPYTFGGGTKLEIK\",\n",
    "]\n",
    "embeddings = antiberty.embed(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([114, 512])\n"
     ]
    }
   ],
   "source": [
    "print(embeddings[1].size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prot trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "import torch\n",
    "import re\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "\n",
    "# Load the model\n",
    "model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\").to(device)\n",
    "\n",
    "# only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)\n",
    "model.to(torch.float32)\n",
    "\n",
    "# prepare your protein sequences as a list\n",
    "sequence_examples = [\"PRTEINO\", \"SEQWENCE\"]\n",
    "\n",
    "# replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids\n",
    "sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n",
    "\n",
    "# tokenize sequences and pad up to the longest sequence in the batch\n",
    "ids = tokenizer(sequence_examples, add_special_tokens=True, padding=\"longest\")\n",
    "\n",
    "input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "\n",
    "# generate embeddings\n",
    "with torch.no_grad():\n",
    "    embedding_repr = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# extract residue embeddings for the first ([0,:]) sequence in the batch and remove padded & special tokens ([0,:7])\n",
    "emb_0 = embedding_repr.last_hidden_state[0,:7] # shape (7 x 1024)\n",
    "# same for the second ([1,:]) sequence but taking into account different sequence lengths ([1,:8])\n",
    "emb_1 = embedding_repr.last_hidden_state[1,:8] # shape (8 x 1024)\n",
    "\n",
    "# if you want to derive a single representation (per-protein embedding) for the whole protein\n",
    "emb_0_per_protein = emb_0.mean(dim=0) # shape (1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(emb_0.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "import json\n",
    "import ablang2\n",
    "import numpy as np\n",
    "import torch\n",
    "import typer\n",
    "from transformers import BertModel, BertTokenizer, T5EncoderModel, T5Tokenizer\n",
    "\n",
    "app = typer.Typer(add_completion=False)\n",
    "\n",
    "def create_embeddings(\n",
    "        dataset_dict:Dict,\n",
    "        save_path : Path=Path(\"/home/gathenes/paratope_model/test/test3/results\"),\n",
    "\n",
    "    ):\n",
    "    \"\"\"Create LLM amino acid embeddings.\n",
    "\n",
    "    Args:\n",
    "        dataset_dict_path (Dict): Dictionary mapping index to heavy and light aa sequence.\n",
    "        save_path (Path): Path where to save embeddings.\n",
    "    \"\"\"\n",
    "    print(\"CREATING EMBEDDINGS\")\n",
    "    sequence_heavy_emb = [dataset_dict[index][\"H_id sequence\"] for index in dataset_dict]\n",
    "    sequence_light_emb = [dataset_dict[index][\"L_id sequence\"] for index in dataset_dict]\n",
    "    ########################################################\n",
    "    ######################## BERT ##########################\n",
    "    ########################################################\n",
    "    bert_tokeniser = BertTokenizer.from_pretrained(\"Exscientia/IgBert\", do_lower_case=False)\n",
    "    bert_model = BertModel.from_pretrained(\"Exscientia/IgBert\", add_pooling_layer=False)\n",
    "    paired_sequences = []\n",
    "    for seq_heavy, seq_light in zip(sequence_heavy_emb, sequence_light_emb):\n",
    "        paired_sequences.append(\n",
    "            \" \".join(seq_heavy) + \" [SEP] \" + \" \".join(seq_light)\n",
    "        )\n",
    "    tokens = bert_tokeniser.batch_encode_plus(\n",
    "        paired_sequences,\n",
    "        add_special_tokens=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=280,\n",
    "        return_tensors=\"pt\",\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        output = bert_model(\n",
    "            input_ids=tokens[\"input_ids\"], attention_mask=tokens[\"attention_mask\"]\n",
    "        )\n",
    "        bert_residue_embeddings = output.last_hidden_state\n",
    "    ########################################################\n",
    "    ###################### IGT5 ############################\n",
    "    ########################################################\n",
    "    igt5_tokeniser = T5Tokenizer.from_pretrained(\"Exscientia/IgT5\", do_lower_case=False)\n",
    "    igt5_model = T5EncoderModel.from_pretrained(\"Exscientia/IgT5\")\n",
    "\n",
    "    tokens = igt5_tokeniser.batch_encode_plus(\n",
    "        paired_sequences,\n",
    "        add_special_tokens=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=280,\n",
    "        return_tensors=\"pt\",\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        output = igt5_model(\n",
    "            input_ids=tokens[\"input_ids\"], attention_mask=tokens[\"attention_mask\"]\n",
    "        )\n",
    "        igt5_residue_embeddings = output.last_hidden_state\n",
    "    ########################################################\n",
    "    ##################### ABLANG ###########################\n",
    "    ########################################################\n",
    "    ablang = ablang2.pretrained()\n",
    "    all_seqs=[]\n",
    "    for seq_heavy, seq_light in zip(sequence_heavy_emb, sequence_light_emb):\n",
    "        all_seqs.append(\n",
    "            [seq_heavy,seq_light]\n",
    "        )\n",
    "    ablang_embeddings=ablang(all_seqs, mode='rescoding', stepwise_masking = False)\n",
    "    ablang_embeddings = [np.pad(each, ((0, 280-each.shape[0]),(0,0)), 'constant', constant_values=((0,0),(0,0))) for each in ablang_embeddings]\n",
    "    ablang_embeddings = torch.Tensor(np.stack(ablang_embeddings))\n",
    "    residue_embeddings = torch.cat([bert_residue_embeddings, igt5_residue_embeddings, ablang_embeddings], dim=2)\n",
    "    torch.save(residue_embeddings, save_path)\n",
    "    return residue_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"/home/gathenes/paratope_model/test/test3/test/dict.json\") as f:\n",
    "    test_dict = json.load(f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
