{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "\n",
    "# Load ESM-2 model\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "# Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)\n",
    "data = [\n",
    "    (\"protein1\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"),\n",
    "    (\"protein2\", \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
    "    (\"protein3\",  \"K A <mask> I S Q\"),\n",
    "]\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "# Extract per-residue representations (on CPU)\n",
    "with torch.no_grad():\n",
    "    results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "token_representations = results[\"representations\"][33]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "print(len(\"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 73, 1280])\n"
     ]
    }
   ],
   "source": [
    "print(token_representations.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antiberty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from antiberty import AntiBERTyRunner\n",
    "\n",
    "antiberty = AntiBERTyRunner()\n",
    "\n",
    "sequences = [\n",
    "    \"EVQLVQSGPEVKKPGTSVKVSCKASGFTFMSSAVQWVRQARGQRLEWIGWIVIGSGNTNYAQKFQERVTITRDMSTSTAYMELSSLRSEDTAVYYCAAPYCSSISCNDGFDIWGQGTMVTVS\",\n",
    "    \"DVVMTQTPFSLPVSLGDQASISCRSSQSLVHSNGNTYLHWYLQKPGQSPKLLIYKVSNRFSGVPDRFSGSGSGTDFTLKISRVEAEDLGVYFCSQSTHVPYTFGGGTKLEIK\",\n",
    "]\n",
    "embeddings = antiberty.embed(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([114, 512])\n"
     ]
    }
   ],
   "source": [
    "print(embeddings[1].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    }
   ],
   "source": [
    "print(len(\"DVVMTQTPFSLPVSLGDQASISCRSSQSLVHSNGNTYLHWYLQKPGQSPKLLIYKVSNRFSGVPDRFSGSGSGTDFTLKISRVEAEDLGVYFCSQSTHVPYTFGGGTKLEIK\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prot trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "import torch\n",
    "import re\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "\n",
    "# Load the model\n",
    "model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\").to(device)\n",
    "\n",
    "# only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)\n",
    "model.to(torch.float32)\n",
    "\n",
    "# prepare your protein sequences as a list\n",
    "sequence_examples = [\"PRTEINO\", \"SEQWENCE\"]\n",
    "\n",
    "# replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids\n",
    "sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n",
    "\n",
    "# tokenize sequences and pad up to the longest sequence in the batch\n",
    "ids = tokenizer(sequence_examples, add_special_tokens=True, padding=\"longest\")\n",
    "\n",
    "input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "\n",
    "# generate embeddings\n",
    "with torch.no_grad():\n",
    "    embedding_repr = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# extract residue embeddings for the first ([0,:]) sequence in the batch and remove padded & special tokens ([0,:7])\n",
    "emb_0 = embedding_repr.last_hidden_state[0,:7] # shape (7 x 1024)\n",
    "# same for the second ([1,:]) sequence but taking into account different sequence lengths ([1,:8])\n",
    "emb_1 = embedding_repr.last_hidden_state[1,:8] # shape (8 x 1024)\n",
    "\n",
    "# if you want to derive a single representation (per-protein embedding) for the whole protein\n",
    "emb_0_per_protein = emb_0.mean(dim=0) # shape (1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(emb_0.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "import json\n",
    "import ablang2\n",
    "import numpy as np\n",
    "import torch\n",
    "import typer\n",
    "from transformers import BertModel, BertTokenizer, T5EncoderModel, T5Tokenizer\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "from antiberty import AntiBERTyRunner\n",
    "\n",
    "\n",
    "\n",
    "app = typer.Typer(add_completion=False)\n",
    "def create_embeddings_3(\n",
    "        dataset_dict: Dict,\n",
    "        save_path: Path = Path(\"/home/gathenes/paratope_model/test/test3/results\"),\n",
    "    ):\n",
    "    \"\"\"Create LLM amino acid embeddings.\n",
    "\n",
    "    Args:\n",
    "        dataset_dict (Dict): Dictionary mapping index to heavy and light aa sequence.\n",
    "        save_path (Path): Path where to save embeddings.\n",
    "    \"\"\"\n",
    "    print(\"CREATING EMBEDDINGS\")\n",
    "    sequence_heavy_emb = [dataset_dict[index][\"H_id sequence\"] for index in dataset_dict]\n",
    "    sequence_light_emb = [dataset_dict[index][\"L_id sequence\"] for index in dataset_dict]\n",
    "    paired_sequences = []\n",
    "    for seq_heavy, seq_light in zip(sequence_heavy_emb, sequence_light_emb):\n",
    "        paired_sequences.append(\n",
    "            \" \".join(seq_heavy) + \" [SEP] \" + \" \".join(seq_light)\n",
    "        )\n",
    "\n",
    "    ########################################################\n",
    "    ######################## BERT ##########################\n",
    "    ########################################################\n",
    "    bert_tokeniser = BertTokenizer.from_pretrained(\"Exscientia/IgBert\", do_lower_case=False)\n",
    "    bert_model = BertModel.from_pretrained(\"Exscientia/IgBert\", add_pooling_layer=False)\n",
    "    tokens = bert_tokeniser.batch_encode_plus(\n",
    "        paired_sequences,\n",
    "        add_special_tokens=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=280,\n",
    "        return_tensors=\"pt\",\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        output = bert_model(\n",
    "            input_ids=tokens[\"input_ids\"], attention_mask=tokens[\"attention_mask\"]\n",
    "        )\n",
    "        bert_residue_embeddings = output.last_hidden_state\n",
    "\n",
    "    ########################################################\n",
    "    ###################### IGT5 ############################\n",
    "    ########################################################\n",
    "    igt5_tokeniser = T5Tokenizer.from_pretrained(\"Exscientia/IgT5\", do_lower_case=False)\n",
    "    igt5_model = T5EncoderModel.from_pretrained(\"Exscientia/IgT5\")\n",
    "    tokens = igt5_tokeniser.batch_encode_plus(\n",
    "        paired_sequences,\n",
    "        add_special_tokens=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=280,\n",
    "        return_tensors=\"pt\",\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        output = igt5_model(\n",
    "            input_ids=tokens[\"input_ids\"], attention_mask=tokens[\"attention_mask\"]\n",
    "        )\n",
    "        igt5_residue_embeddings = output.last_hidden_state\n",
    "\n",
    "    ########################################################\n",
    "    ##################### ABLANG ###########################\n",
    "    ########################################################\n",
    "    ablang = ablang2.pretrained()\n",
    "    all_seqs = [[seq_heavy, seq_light] for seq_heavy, seq_light in zip(sequence_heavy_emb, sequence_light_emb)]\n",
    "    ablang_embeddings = ablang(all_seqs, mode='rescoding', stepwise_masking=False)\n",
    "    ablang_embeddings = [np.pad(each, ((0, 280 - each.shape[0]), (0, 0)), 'constant') for each in ablang_embeddings]\n",
    "    ablang_embeddings = torch.Tensor(np.stack(ablang_embeddings))\n",
    "\n",
    "    ########################################################\n",
    "    ######################## ESM ###########################\n",
    "    ########################################################\n",
    "    esm_model, esm_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "    esm_batch_converter = esm_alphabet.get_batch_converter()\n",
    "    esm_model.eval()\n",
    "\n",
    "    data = []\n",
    "    for seq_heavy, seq_light in zip(sequence_heavy_emb, sequence_light_emb):\n",
    "        data.append((\"ab\", \"\".join(seq_heavy) + \"\".join(seq_light)))\n",
    "    _, _, esm_batch_tokens = esm_batch_converter(data)\n",
    "    with torch.no_grad():\n",
    "        esm_results = esm_model(esm_batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "    esm_embeddings = esm_results[\"representations\"][33]\n",
    "    pad_length = 280 - esm_embeddings.size(1)  # 280 is the desired length\n",
    "    padding = (0, 0, 0, pad_length)\n",
    "    esm_embeddings = F.pad(esm_embeddings, padding, mode='constant', value=0)\n",
    "\n",
    "    ########################################################\n",
    "    #################### ANTIBERTY #########################\n",
    "    ########################################################\n",
    "    antiberty = AntiBERTyRunner()\n",
    "    antiberty_sequences = [\n",
    "        \"\".join(seq_heavy) + \"\".join(seq_light)\n",
    "        for seq_heavy, seq_light in zip(sequence_heavy_emb, sequence_light_emb)\n",
    "    ]\n",
    "    antiberty_embeddings = antiberty.embed(antiberty_sequences)\n",
    "    antiberty_embeddings = [np.pad(each, ((0, 280 - each.shape[0]), (0, 0)), 'constant') for each in antiberty_embeddings]\n",
    "    antiberty_embeddings = torch.Tensor(np.stack(antiberty_embeddings))\n",
    "\n",
    "    ########################################################\n",
    "    ####################### ProtT5 #########################\n",
    "    ########################################################\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    prot_t5_tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\", do_lower_case=False)\n",
    "    prot_t5_model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\").to(device)\n",
    "    prot_t5_model.to(torch.float32)\n",
    "\n",
    "    prot_t5_sequences = [\n",
    "        \"\".join(seq_heavy) + \"\".join(seq_light)\n",
    "        for seq_heavy, seq_light in zip(sequence_heavy_emb, sequence_light_emb)\n",
    "    ]\n",
    "    prot_t5_sequences = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", seq))) for seq in prot_t5_sequences]\n",
    "    prot_t5_ids = prot_t5_tokenizer(prot_t5_sequences, add_special_tokens=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "    input_ids = prot_t5_ids['input_ids'].to(device)\n",
    "    attention_mask = prot_t5_ids['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prot_t5_output = prot_t5_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    prot_t5_embeddings = prot_t5_output.last_hidden_state\n",
    "    pad_length = 280 - prot_t5_embeddings.size(1)\n",
    "    padding = (0, 0, 0, pad_length)\n",
    "    prot_t5_embeddings = F.pad(prot_t5_embeddings, padding, mode='constant', value=0)\n",
    "\n",
    "    ########################################################\n",
    "    ################# CONCATENATE EMBEDDINGS ###############\n",
    "    ########################################################\n",
    "    residue_embeddings = torch.cat([\n",
    "        bert_residue_embeddings,\n",
    "        igt5_residue_embeddings,\n",
    "        ablang_embeddings,\n",
    "        esm_embeddings,\n",
    "        antiberty_embeddings,\n",
    "        prot_t5_embeddings\n",
    "    ], dim=2)\n",
    "\n",
    "    return (\n",
    "        bert_residue_embeddings,\n",
    "        igt5_residue_embeddings,\n",
    "        ablang_embeddings,\n",
    "        esm_embeddings,\n",
    "        antiberty_embeddings,\n",
    "        prot_t5_embeddings,\n",
    "        residue_embeddings\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"/home/gathenes/paratope_model/test/test3/test/dict.json\") as f:\n",
    "    test_dict = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING EMBEDDINGS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  5.68it/s]\n",
      "/home/gathenes/.pyenv/versions/3.10.13/envs/embeddings_310/lib/python3.10/site-packages/ablang2/load_model.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\n"
     ]
    }
   ],
   "source": [
    "bert_residue_embeddings, igt5_residue_embeddings, ablang_embeddings, esm_embeddings, antiberty_embeddings, prot_t5_embeddings, residue_embeddings= create_embeddings_3(test_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 280, 1024])\n",
      "torch.Size([3, 280, 1024])\n",
      "torch.Size([3, 280, 480])\n",
      "torch.Size([3, 280, 1280])\n",
      "torch.Size([3, 280, 512])\n",
      "torch.Size([3, 280, 1024])\n"
     ]
    }
   ],
   "source": [
    "for each in [bert_residue_embeddings, igt5_residue_embeddings, ablang_embeddings, esm_embeddings, antiberty_embeddings, prot_t5_embeddings]:\n",
    "    print(each.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 280, 5344])\n"
     ]
    }
   ],
   "source": [
    "print(residue_embeddings.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_dataset import add_convex_hull_column\n",
    "from utils import read_pdb_to_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [record_name, atom_number, blank_1, atom_name, alt_loc, residue_name, blank_2, chain_id, residue_number, insertion, blank_3, x_coord, y_coord, z_coord, occupancy, b_factor, blank_4, segment_id, element_symbol, charge, line_idx, IMGT]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "chains=[\"B\",\"C\"]\n",
    "df_pdb = (\n",
    "    read_pdb_to_dataframe(\"/home/gathenes/all_structures/imgt_renumbered_pecan/5bv7.pdb\")\n",
    "    .query(\"chain_id.isin(@chains) and residue_number<129\")\n",
    ")\n",
    "print(df_pdb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pdb = add_convex_hull_column(df_pdb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
